{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav_Gola\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import bz2\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk # standard preprocessing\n",
    "import operator # sorting items in dictionary by value\n",
    "#nltk.download() #tokenizers/punkt/PY3/english.pickle\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Generating_Batches_of_Data_Skip_Gram_ import *\n",
    "from data_download import *\n",
    "from read_data import *\n",
    "from Build_dictionary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified wikipedia2text-extracted.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "filename = maybe_download(url,'wikipedia2text-extracted.txt.bz2', 18377035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 11631723\n",
      "Example words (start):  ['Propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['useless', 'for', 'cultivation', '.', 'and', 'people', 'have', 'sex', 'there', '.']\n"
     ]
    }
   ],
   "source": [
    "# Without preprocessing \n",
    "\n",
    "words = read_data_without_preprocess(filename=filename)\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3360286\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "# with preprocess\n",
    "words = read_data_with_preprocess(filename=filename)\n",
    "\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dictionary\n",
    "\n",
    "#Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\"\n",
    "    \n",
    "#dictionary: maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school:4})\n",
    "#reverse_dictionary: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
    "#count: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
    "#data : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we restrict our vocabulary size to 50000\n",
    "vocabulary_size = 50000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 69215], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
      "Sample data [1721, 9, 8, 16471, 223, 4, 5165, 4456, 26, 11590]\n"
     ]
    }
   ],
   "source": [
    "# Dictionary build is made of most common words.\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words,vocabulary_size=vocabulary_size)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Batches of Data for Skip-Gram\n",
    "Generates a batch or target words (batch) and a batch of corresponding context words (labels). It reads 2(  window_size)+1 words at a time (called a span) and create 2*(window_size) datapoints in a single span. The function continue in this manner until batch_size datapoints are created. Everytime we reach the end of the word sequence, we start from beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "with window_size = 1:\n",
      "    batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "    labels: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['influencing', 'influencing', 'influencing', 'influencing', 'the', 'the', 'the', 'the']\n",
      "    labels: ['aimed', 'at', 'the', 'opinions', 'at', 'influencing', 'opinions', 'or']\n"
     ]
    }
   ],
   "source": [
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "for window_size in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_skip_gram(batch_size=8, window_size=window_size,data=data)\n",
    "    print('\\nwith window_size = %d:' %window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Batch_size = 16 \n",
    "embedding_size = 128\n",
    "window_size = 5\n",
    "vocabulary_size = 50000 \n",
    "\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors\n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "# We sample valid datapoints randomly from a large window without always being deterministic\n",
    "\n",
    "valid_window = 50\n",
    "# When selecting valid examples, we select some of the most frequent words as well as\n",
    "# some moderately rare words as well\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[Batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[Batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "embedding_space = tf.Variable(tf.random_uniform(shape=[vocabulary_size,embedding_size],minval=-1.0,maxval=1.0,dtype=tf.float32,name='Embedding_layer/Embedding_space'))\n",
    "\n",
    "softmax_weight = tf.Variable(tf.truncated_normal(shape=[vocabulary_size,embedding_size],stddev=0.5 / math.sqrt(embedding_size)))\n",
    "\n",
    "softmax_bias = tf.Variable(tf.random_uniform(shape=[vocabulary_size],minval=0.0,maxval=0.01))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Lookup Step\n",
    "#Looking for a particular embedding in embedding space (V * D matrix)\n",
    "\n",
    "We first defing a lookup function to fetch the corresponding embedding vectors for a set of given inputs. With that, we define negative sampling loss function tf.nn.sampled_softmax_loss which takes in the embedding vectors and previously defined neural network parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(embedding_space, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Sampling Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weight,biases=softmax_bias,inputs=embed,labels=train_labels,num_sampled=num_sampled,num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 2000: 4.646618\n",
      "Average loss at step 4000: 4.248596\n",
      "Average loss at step 6000: 4.181313\n",
      "Average loss at step 8000: 4.347678\n",
      "Average loss at step 10000: 4.164783\n",
      "Average loss at step 12000: 4.231609\n",
      "Average loss at step 14000: 4.359489\n",
      "Average loss at step 16000: 4.292942\n",
      "Average loss at step 18000: 4.376451\n",
      "Average loss at step 20000: 4.279770\n",
      "Average loss at step 22000: 4.536325\n",
      "Average loss at step 24000: 4.191920\n",
      "Average loss at step 26000: 4.287468\n",
      "Average loss at step 28000: 4.266125\n",
      "Average loss at step 30000: 4.280929\n",
      "Average loss at step 32000: 4.377006\n",
      "Average loss at step 34000: 4.304320\n",
      "Average loss at step 36000: 4.424525\n",
      "Average loss at step 38000: 4.289006\n",
      "Average loss at step 40000: 4.380775\n",
      "Average loss at step 42000: 4.403068\n",
      "Average loss at step 44000: 4.369575\n",
      "Average loss at step 46000: 4.376791\n",
      "Average loss at step 48000: 4.317907\n",
      "Average loss at step 50000: 4.419596\n",
      "Average loss at step 52000: 4.454362\n",
      "Average loss at step 54000: 4.342832\n",
      "Average loss at step 56000: 4.435581\n",
      "Average loss at step 58000: 4.197353\n",
      "Average loss at step 60000: 4.510939\n",
      "Average loss at step 62000: 4.378177\n",
      "Average loss at step 64000: 4.440684\n",
      "Average loss at step 66000: 4.353904\n",
      "Average loss at step 68000: 4.410198\n",
      "Average loss at step 70000: 4.340429\n",
      "Average loss at step 72000: 4.403017\n",
      "Average loss at step 74000: 4.292325\n",
      "Average loss at step 76000: 4.331808\n",
      "Average loss at step 78000: 4.500303\n",
      "Average loss at step 80000: 4.290954\n",
      "Average loss at step 82000: 4.172064\n",
      "Average loss at step 84000: 4.273334\n",
      "Average loss at step 86000: 4.282226\n",
      "Average loss at step 88000: 4.264417\n",
      "Average loss at step 90000: 4.385635\n",
      "Average loss at step 92000: 4.296796\n",
      "Average loss at step 94000: 4.344861\n",
      "Average loss at step 96000: 4.260913\n",
      "Average loss at step 98000: 4.338252\n",
      "Average loss at step 100000: 4.252156\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "skip_losses = []\n",
    "# ConfigProto is a way of providing various configuration settings\n",
    "# Tensorflow Automatically chooses the device\n",
    "# required to execute the graph\n",
    "\n",
    "batch_size =16\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
    "    # Initialize the variables in the graph\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "\n",
    "  # Train the Word2vec model for num_step iterations\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        # Generate a single batch of data\n",
    "        batch_data, batch_labels = generate_batch_skip_gram(\n",
    "          batch_size, window_size,data=data)\n",
    "\n",
    "        # Populate the feed_dict and run the optimizer (minimize loss)\n",
    "        # and compute the loss\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        # Update the average loss variable\n",
    "        average_loss += l\n",
    "\n",
    "        if (step+1) % 2000== 0 and step>0 :\n",
    "            average_loss = average_loss / 2000\n",
    "            skip_losses.append(average_loss)\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "            average_loss = 0\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "\n",
    "    saver.save(sess=session,save_path = 'C:\\\\Users\\\\Gaurav_Gola\\\\Desktop\\\\Practice\\\\Word_2_vec\\\\Model_saved\\\\word_2vec_trained')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
